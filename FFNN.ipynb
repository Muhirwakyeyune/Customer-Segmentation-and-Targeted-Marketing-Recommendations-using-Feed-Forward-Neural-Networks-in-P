{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Zv0muaqOr8E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "xacwJD5-PYqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step 2 load iris datasets and split into test and train sets\n",
        "iris=load_iris()\n",
        "#split the data\n",
        "X_train, X_test, y_train, y_test=train_test_split(iris.data, iris.target,test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "1zYCUkX0V0KU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Next, we'll define the neural network \n",
        "  architecture. In this case, we'll use a simple\n",
        " architecture with two hidden layers, each with 10\n",
        "  neurons, and a single output layer with three\n",
        " neurons (one for each type of Iris flower):  \"\"\"\n",
        " "
      ],
      "metadata": {
        "id": "COvDRzzrWyQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 3 define Neural Networks\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.fc1=nn.Linear(4,10)\n",
        "    self.fc2=nn.Linear(10,10)\n",
        "    self.fc3=nn.Linear(10,3)\n",
        "  def forward(self, x):\n",
        "    x=torch.relu(self.fc1(x))\n",
        "    x=torch.relu(self.fc2(x))\n",
        "    x=self.fc3(x)\n",
        "    return x   \n",
        "\n"
      ],
      "metadata": {
        "id": "1ScoO0TBXBZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net=Net()\n",
        "# we define the loss function and criterion to train the network\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(net.parameters(), lr=0.0002)"
      ],
      "metadata": {
        "id": "WlkpStovYPim"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we train. using Minibatch gradient descent\n",
        "batch_size=32\n",
        "num_epochs=1000\n",
        "for epoch in range(num_epochs):\n",
        "  running_loss=0.0\n",
        "  for i in range(0, len(X_test), batch_size):\n",
        "    # get mini batch input and targets\n",
        "    inputs=torch.tensor(X_train[i: i + batch_size], dtype=torch.float32)\n",
        "    targets=torch.tensor(y_train[i: i + batch_size],dtype=torch.long)\n",
        "    # zero parameters gradients\n",
        "    optimizer.zero_grad()\n",
        "    # forward + backward + optimizer\n",
        "    outputs=net(inputs)\n",
        "    loss=criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step() \n",
        "    # print stats\n",
        "    running_loss +=loss.item()\n",
        "  if ( epoch +1)% 10==0:\n",
        "    print('Epoch [%d/%d], Loss : %.4f'%(epoch +1, num_epochs,running_loss))  \n"
      ],
      "metadata": {
        "id": "jPm-aUtNYhLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5112cbf-9708-4e70-d3c7-6151b3e1b316"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss : 0.3908\n",
            "Epoch [20/1000], Loss : 0.3872\n",
            "Epoch [30/1000], Loss : 0.3836\n",
            "Epoch [40/1000], Loss : 0.3800\n",
            "Epoch [50/1000], Loss : 0.3765\n",
            "Epoch [60/1000], Loss : 0.3731\n",
            "Epoch [70/1000], Loss : 0.3697\n",
            "Epoch [80/1000], Loss : 0.3663\n",
            "Epoch [90/1000], Loss : 0.3630\n",
            "Epoch [100/1000], Loss : 0.3596\n",
            "Epoch [110/1000], Loss : 0.3564\n",
            "Epoch [120/1000], Loss : 0.3531\n",
            "Epoch [130/1000], Loss : 0.3499\n",
            "Epoch [140/1000], Loss : 0.3467\n",
            "Epoch [150/1000], Loss : 0.3435\n",
            "Epoch [160/1000], Loss : 0.3403\n",
            "Epoch [170/1000], Loss : 0.3372\n",
            "Epoch [180/1000], Loss : 0.3341\n",
            "Epoch [190/1000], Loss : 0.3310\n",
            "Epoch [200/1000], Loss : 0.3279\n",
            "Epoch [210/1000], Loss : 0.3249\n",
            "Epoch [220/1000], Loss : 0.3218\n",
            "Epoch [230/1000], Loss : 0.3188\n",
            "Epoch [240/1000], Loss : 0.3158\n",
            "Epoch [250/1000], Loss : 0.3129\n",
            "Epoch [260/1000], Loss : 0.3099\n",
            "Epoch [270/1000], Loss : 0.3070\n",
            "Epoch [280/1000], Loss : 0.3041\n",
            "Epoch [290/1000], Loss : 0.3012\n",
            "Epoch [300/1000], Loss : 0.2983\n",
            "Epoch [310/1000], Loss : 0.2954\n",
            "Epoch [320/1000], Loss : 0.2925\n",
            "Epoch [330/1000], Loss : 0.2896\n",
            "Epoch [340/1000], Loss : 0.2868\n",
            "Epoch [350/1000], Loss : 0.2839\n",
            "Epoch [360/1000], Loss : 0.2811\n",
            "Epoch [370/1000], Loss : 0.2783\n",
            "Epoch [380/1000], Loss : 0.2755\n",
            "Epoch [390/1000], Loss : 0.2727\n",
            "Epoch [400/1000], Loss : 0.2699\n",
            "Epoch [410/1000], Loss : 0.2672\n",
            "Epoch [420/1000], Loss : 0.2644\n",
            "Epoch [430/1000], Loss : 0.2617\n",
            "Epoch [440/1000], Loss : 0.2590\n",
            "Epoch [450/1000], Loss : 0.2563\n",
            "Epoch [460/1000], Loss : 0.2536\n",
            "Epoch [470/1000], Loss : 0.2510\n",
            "Epoch [480/1000], Loss : 0.2483\n",
            "Epoch [490/1000], Loss : 0.2457\n",
            "Epoch [500/1000], Loss : 0.2430\n",
            "Epoch [510/1000], Loss : 0.2405\n",
            "Epoch [520/1000], Loss : 0.2379\n",
            "Epoch [530/1000], Loss : 0.2353\n",
            "Epoch [540/1000], Loss : 0.2328\n",
            "Epoch [550/1000], Loss : 0.2303\n",
            "Epoch [560/1000], Loss : 0.2277\n",
            "Epoch [570/1000], Loss : 0.2253\n",
            "Epoch [580/1000], Loss : 0.2228\n",
            "Epoch [590/1000], Loss : 0.2203\n",
            "Epoch [600/1000], Loss : 0.2179\n",
            "Epoch [610/1000], Loss : 0.2155\n",
            "Epoch [620/1000], Loss : 0.2131\n",
            "Epoch [630/1000], Loss : 0.2107\n",
            "Epoch [640/1000], Loss : 0.2083\n",
            "Epoch [650/1000], Loss : 0.2060\n",
            "Epoch [660/1000], Loss : 0.2036\n",
            "Epoch [670/1000], Loss : 0.2013\n",
            "Epoch [680/1000], Loss : 0.1991\n",
            "Epoch [690/1000], Loss : 0.1968\n",
            "Epoch [700/1000], Loss : 0.1946\n",
            "Epoch [710/1000], Loss : 0.1923\n",
            "Epoch [720/1000], Loss : 0.1901\n",
            "Epoch [730/1000], Loss : 0.1879\n",
            "Epoch [740/1000], Loss : 0.1858\n",
            "Epoch [750/1000], Loss : 0.1836\n",
            "Epoch [760/1000], Loss : 0.1815\n",
            "Epoch [770/1000], Loss : 0.1794\n",
            "Epoch [780/1000], Loss : 0.1773\n",
            "Epoch [790/1000], Loss : 0.1753\n",
            "Epoch [800/1000], Loss : 0.1732\n",
            "Epoch [810/1000], Loss : 0.1712\n",
            "Epoch [820/1000], Loss : 0.1693\n",
            "Epoch [830/1000], Loss : 0.1673\n",
            "Epoch [840/1000], Loss : 0.1654\n",
            "Epoch [850/1000], Loss : 0.1635\n",
            "Epoch [860/1000], Loss : 0.1616\n",
            "Epoch [870/1000], Loss : 0.1597\n",
            "Epoch [880/1000], Loss : 0.1579\n",
            "Epoch [890/1000], Loss : 0.1561\n",
            "Epoch [900/1000], Loss : 0.1543\n",
            "Epoch [910/1000], Loss : 0.1525\n",
            "Epoch [920/1000], Loss : 0.1508\n",
            "Epoch [930/1000], Loss : 0.1490\n",
            "Epoch [940/1000], Loss : 0.1473\n",
            "Epoch [950/1000], Loss : 0.1456\n",
            "Epoch [960/1000], Loss : 0.1440\n",
            "Epoch [970/1000], Loss : 0.1423\n",
            "Epoch [980/1000], Loss : 0.1407\n",
            "Epoch [990/1000], Loss : 0.1391\n",
            "Epoch [1000/1000], Loss : 0.1375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the network\n",
        "correct=0\n",
        "total=0\n",
        "with torch.no_grad():\n",
        "  #\n",
        "  inputs=torch.tensor(X_test, dtype=torch.float32)\n",
        "  targets=torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "  #\n",
        "  outputs=net(inputs)\n",
        "  _, predicted=torch.max(outputs.data, 1)\n",
        "  accuracy =(predicted==targets).sum().item()/len(targets)\n",
        "  print(\" accuracy : %.2f\"%(accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP6lYPCnyvys",
        "outputId": "20fad6cd-0fcc-4c4c-db74-f21bdead64dd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " accuracy : 100.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OwhCo-EE3F2F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}